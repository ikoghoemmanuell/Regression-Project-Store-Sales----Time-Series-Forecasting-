{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title\n",
    "Regression Project (Store Sales -- Time Series Forecasting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Description\n",
    "This is a time series forecasting problem. In this project, we will predict store sales on data from Corporation Favorita, a large Ecuadorian-based grocery retailer.\n",
    "\n",
    "Specifically, we are to build a model that more accurately predicts the unit sales for thousands of items sold at different Favorita stores.\n",
    "\n",
    "The training data includes dates, store, and product information, whether that item was being promoted, as well as the sales numbers. Additional files include supplementary information that may be useful in building your models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis\n",
    "## Null Hypothesis, HO\n",
    "Series is non-stationary\n",
    "## AlternativeHypothesis, H1\n",
    "Series is stationary\n",
    "# Questions\n",
    "1. Is the train dataset complete (has all the required dates)?\n",
    "\n",
    "2. Which dates have the lowest and highest sales for each year?\n",
    "\n",
    "3. Did the earthquake impact sales?\n",
    "\n",
    "4. Are certain groups of stores selling more products? (Cluster, city, state, type)\n",
    "\n",
    "5. Are sales affected by promotions, oil prices and holidays?\n",
    "\n",
    "6. What analysis can we get from the date and its extractable features?\n",
    "\n",
    "7. What is the difference between RMSLE, RMSE, MSE (or why is the MAE greater than all of them?)\n",
    "\n",
    "ADDITIONAL QUESTIONS\n",
    "\n",
    "8. What is the trend of sales over time?\n",
    "9. What is the trend of transactions over time?\n",
    "10. Highest and lowest performing stores in terms of sales\n",
    "11. Highest performing family of products\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.dates as mdates\n",
    "from pandas_profiling import ProfileReport\n",
    "import plotly.graph_objects as go\n",
    "%matplotlib inline\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.stattools import kpss\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays=pd.read_csv(\"store-sales-time-series-forecasting/holidays_events.csv\")\n",
    "oil=pd.read_csv(\"store-sales-time-series-forecasting/oil.csv\")\n",
    "sample=pd.read_csv(\"store-sales-time-series-forecasting/sample_submission.csv\")\n",
    "stores=pd.read_csv(\"store-sales-time-series-forecasting/stores.csv\")\n",
    "test=pd.read_csv(\"store-sales-time-series-forecasting/test.csv\")\n",
    "train=pd.read_csv(\"store-sales-time-series-forecasting/train.csv\", parse_dates =['date'])\n",
    "transactions=pd.read_csv(\"store-sales-time-series-forecasting/transactions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis: EDA\n",
    "# Dataset overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# profile_data = ProfileReport(train, title =\"Profiling Report\")\n",
    "# profile_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our date starts from January 2013 till October 2017\n",
    "\n",
    "Sales has a strong positive correlation with onpromotion, so we'll focus on sales, id and onpromotion since they correlate with one another the most\n",
    "\n",
    "No missing values in train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see that the date column is in object instead of datetime\n",
    "\n",
    "so we convert the datatypes so we can explore them further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting date columns to datetime\n",
    "def to_dateTime(df):\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "to_dateTime(transactions)\n",
    "to_dateTime(test)\n",
    "to_dateTime(oil)\n",
    "to_dateTime(holidays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#indexing our date column\n",
    "train=train.set_index(['date'])\n",
    "test=test.set_index(['date'])\n",
    "transactions=transactions.set_index(['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['family'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train['family'] = train['family'].apply(\n",
    "#     lambda x: str(x).replace('GROCERY I','GROCERY II') if 'â‚¹' in x\n",
    "# else x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays.tail(),holidays.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop unnecessary columns in holidays\n",
    "holidays.drop(\n",
    "    columns=['locale', 'locale_name'],\n",
    "    inplace=True\n",
    "    )\n",
    "# delete rows with transferred as true\n",
    "transferred_true = holidays[ (holidays['transferred'] == True)].index\n",
    "holidays.drop(transferred_true , inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the id column doesn't give additional info, so we drop it\n",
    "train.drop(columns=['id'], inplace=True)\n",
    "test.drop(columns=['id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oil.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(transactions.isnull().sum())\n",
    "print(test.isnull().sum())\n",
    "print(train.isnull().sum())\n",
    "print(stores.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample.isnull().sum())\n",
    "print(oil.isnull().sum())\n",
    "print(holidays.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "our oil data has 43 missing values, we'll deal with them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_data = oil[oil.isnull().any(axis=1)]\n",
    "null_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oil=oil.dropna()\n",
    "# oil.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merging our data\n",
    "we want to merge the transactions with our train data, so we can use it to train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging transactions with our train data\n",
    "merged=pd.merge(\n",
    "    train.reset_index(), transactions.reset_index(),\n",
    "    how='outer', \n",
    "    on=['date', 'store_nbr']\n",
    "    ).set_index('date')\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging merged with holidays\n",
    "merged2=pd.merge(\n",
    "    merged.reset_index(), holidays,\n",
    "    how='outer', \n",
    "    on=['date']\n",
    "    ).set_index('date')\n",
    "merged2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging merged2 with oil data\n",
    "merged3=pd.merge(\n",
    "    merged2.reset_index(), oil,\n",
    "    how='outer', \n",
    "    on=['date']\n",
    "    ).set_index('date')\n",
    "\n",
    "#renaming our oil column \n",
    "merged3.rename(\n",
    "    columns = {'dcoilwtico':'oil_price'}, \n",
    "    inplace = True\n",
    "    )\n",
    "merged3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Specify the data columns we want to include (i.e. exclude Year, Month, Weekday Name)\n",
    "# data_columns = ['store_nbr', 'sales', 'transactions', 'family', 'onpromotion']\n",
    "# # Resample to daily frequency, aggregating with sum\n",
    "# merged_daily_sum = merged[data_columns].resample('D').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Analysis\n",
    "\n",
    "8. What is the trend of sales over time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualizing sales in train data\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.title('Sales over Time')\n",
    "train['sales'].plot(linewidth = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sales in our train data is seasonal. There is no trend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. What is the trend of transactions over time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=list(transactions.index), y=list(transactions.transactions)))\n",
    "\n",
    "# Set title\n",
    "fig.update_layout(\n",
    "    title_text=\"Transactions over Time\"\n",
    ")\n",
    "\n",
    "# Add range slider\n",
    "fig.update_layout(\n",
    "    xaxis=dict(\n",
    "        rangeselector=dict(\n",
    "            buttons=list([\n",
    "                dict(count=1,\n",
    "                     label=\"1m\",\n",
    "                     step=\"month\",\n",
    "                     stepmode=\"backward\"),\n",
    "                dict(count=6,\n",
    "                     label=\"6m\",\n",
    "                     step=\"month\",\n",
    "                     stepmode=\"backward\"),\n",
    "                dict(count=1,\n",
    "                     label=\"YTD\",\n",
    "                     step=\"year\",\n",
    "                     stepmode=\"todate\"),\n",
    "                dict(count=1,\n",
    "                     label=\"1y\",\n",
    "                     step=\"year\",\n",
    "                     stepmode=\"backward\"),\n",
    "                dict(step=\"all\")\n",
    "            ])\n",
    "        ),\n",
    "        rangeslider=dict(\n",
    "            visible=True\n",
    "        ),\n",
    "        type=\"date\"\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we get a spike in transactions every December 23rd & around May 10th throughout, this shows seasonality, but no trend\n",
    "### ADF Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adfuller(transactions.transactions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Falied to reject Null Hypothesis: Since p-value is 1.85e-29 (<0.05), Series is non-stationary\n",
    "## KPSS Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpss(transactions.transactions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p-value is 0.01(<0.05) meaning our series is non-stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualizing oil prices in merged data\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.title('Oil prices over Time')\n",
    "merged3['oil_price'].plot.area(linewidth = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bivariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Highest and lowest performing stores in terms of sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_stores_sales = (\n",
    "    merged.groupby(\"store_nbr\")[\"sales\"]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    "    .sort_values(by=\"sales\",ascending=False)\n",
    ")\n",
    "\n",
    "top_stores_transactions = (\n",
    "    merged.groupby(\"store_nbr\")[\"transactions\"]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    "    .sort_values(by=\"transactions\",ascending=False)\n",
    ")\n",
    "\n",
    "topSS=top_stores_sales.iloc[:5]\n",
    "topST=top_stores_transactions.iloc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting stores Vs sales and Transactions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 3), sharey=True)\n",
    "fig.suptitle('Top Stores by Sales and Transactions')\n",
    "\n",
    "# Bulbasaur\n",
    "sns.barplot(ax=axes[0], x=topSS.store_nbr, y=topSS.sales)\n",
    "# axes[0].set_title(bulbasaur.name)\n",
    "\n",
    "# Charmander\n",
    "sns.barplot(ax=axes[1], x=topST.store_nbr, y=topST.transactions)\n",
    "# axes[1].set_title(charmander.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Highest performing family of products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_family_by_sales = (\n",
    "    merged.groupby(\"family\")[\"sales\"]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    "    .sort_values(by=\"sales\",ascending=False)\n",
    ")\n",
    "\n",
    "topFS=top_family_by_sales.iloc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting stores Vs sales \n",
    "fig = plt.figure(figsize=(10,3))\n",
    "plt.title(\"Top Family by sales\")\n",
    "sns.barplot(data=topFS, x='family', y='sales', palette='Blues_d')\n",
    "fig.show()\n",
    "\n",
    "sns.set(font_scale = 1)\n",
    "plt.xticks(rotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Issues with the data\n",
    "1. missing values in our oil data before merging\n",
    "2. We need some categorical columns for our model\n",
    "3. The data has a lot of missing values after merging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to fix them\n",
    "1. We leave them for now\n",
    "2. We have to do some encoding\n",
    "3. For time series data, we most likely fill with the value closest to it, but overall, we have to be very careful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Processing & Engineering\n",
    "## Drop Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup = merged3.loc[merged3.duplicated(),:]\n",
    "print(dup.shape)\n",
    "dup.tail(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've looked at the duplicate rows,\n",
    "\n",
    "they contain mostly null values\n",
    "\n",
    "they aren't useful, so we drop them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pandas.DataFrame.drop_duplicates method\n",
    "merged3.drop_duplicates(keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impute Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first, lets have a quick overview before deciding how to handle missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.heatmap(merged3.isnull(), cbar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged3.isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's interpolate the 1st 4 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged3['store_nbr'] = merged3['store_nbr'].interpolate(method='linear')\n",
    "imputer = SimpleImputer(missing_values=np.NaN, strategy='most_frequent')\n",
    "merged3['family'] = imputer.fit_transform(merged3[['family']])\n",
    "merged3['sales'] = merged3['sales'].interpolate(method='time')\n",
    "merged3['onpromotion'] = merged3['onpromotion'].interpolate(method='linear')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since non-holidays are regular days, let's impute these 3 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill missing holiday rows with normal days\n",
    "def replacer(column, text):\n",
    "    imputer = SimpleImputer(missing_values=np.NaN, strategy='constant', fill_value=text)\n",
    "    merged3[column] = imputer.fit_transform(merged3[[column]])\n",
    "\n",
    "replacer('type','Regular')\n",
    "replacer('description','Regular Day')\n",
    "replacer('transferred',False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the remaining columns have more null values, so let's check which method is best\n",
    "\n",
    "by creating function that checks r2 scores of interpolation methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolation_method_checker(df):\n",
    "    # select a series of non-null values\n",
    "    df_not_null = df[~df.isna()]\n",
    "\n",
    "    # Randomly set a percentage of this data as missing\n",
    "    p = 0.4 #percentage missing data required (40 percent)\n",
    "    df = pd.DataFrame(np.random.randint(0,100,size=(10,10)))\n",
    "    mask = np.random.choice([True, False], size=df_not_null.shape, p=[p,1-p])\n",
    "    df_null = df_not_null.mask(mask)\n",
    "    \n",
    "    # Try different interpolation methods\n",
    "    time = df_null.interpolate(method='time')\n",
    "    linear = df_null.interpolate(method='linear')\n",
    "    slinear = df_null.interpolate(method='slinear')\n",
    "    pad = df_null.interpolate(method='pad')\n",
    "\n",
    "    # check r2 scores\n",
    "    from sklearn.metrics import r2_score\n",
    "    t = r2_score(df_not_null, time)\n",
    "    l = r2_score(df_not_null, linear)\n",
    "    sl = r2_score(df_not_null, slinear)\n",
    "    p = r2_score(df_not_null, pad)\n",
    "\n",
    "    print(t,l,sl,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolation_method_checker(merged3['transactions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolation_method_checker(merged3['oil_price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using linear interpolation\n",
    "merged3['transactions'] = merged3['transactions'].interpolate(method='linear')\n",
    "merged3['transactions'] = merged3['transactions'].bfill()\n",
    "\n",
    "merged3['oil_price'] = merged3['oil_price'].interpolate(method='linear')\n",
    "merged3['oil_price'] = merged3['oil_price'].bfill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.heatmap(merged3.isnull(), cbar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged3.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Features Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use pandas' powerful time-based indexing to analyze data\n",
    "test['Year'] = test.index.year\n",
    "test['Month'] = test.index.month\n",
    "test['Weekday Name'] = test.index.day_name()\n",
    "\n",
    "merged3['Year'] = merged3.index.year\n",
    "merged3['Month'] = merged3.index.month\n",
    "merged3['Weekday Name'] = merged3.index.day_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1=merged3\n",
    "train1.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use train_test_split with a random_state, and add stratify for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "train_labels=train1.drop('sales',axis=1)\n",
    "cat_attribs=[\"family\", 'type', 'description', 'transferred', 'Weekday Name']\n",
    "train_num=train1.drop([\"family\", 'type', 'description', 'transferred', 'Weekday Name'], axis=1)\n",
    "num_attribs=list(train_num)\n",
    "\n",
    "\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[(\"scaler\", StandardScaler())]\n",
    ")\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"encoder\", OneHotEncoder())\n",
    "    ]\n",
    ")\n",
    "full_pipeline = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", categorical_transformer, cat_attribs),\n",
    "        (\"num\", numeric_transformer, num_attribs)\n",
    "    ]\n",
    ")\n",
    "train_prepared=full_pipeline.fit_transform(train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prepared.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features Scaling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Model #001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the .fit method"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "7679c2132d3f6ce38c9df14d554b39c06862b36a4e6689c81f9ae15bd0911d7d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
